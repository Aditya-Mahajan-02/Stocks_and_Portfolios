











# import needed libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score





input_path = '../data/clean_data/stocks-clean-data-2.csv'
stocks = pd.read_csv(input_path)





print(stocks.shape)
stocks.head()





stocks.info()











# set a random seed for reproducibility
np.random.seed(2024)


# keep track of metrics for each model iteration on this list
# input format to append {k: value, n_init: value, inertia: value, silhouette: value, remarks: text}
metrics = []





# instantiate standard scaler
sc = StandardScaler()

# scale data ----------------------------------------------------------

# drop non-numeric columns from stocks df
# symbol: 503 unique values, too many to encode
# date: does not make sense to keep as
# there is already a timestamp column in ms
stocks_sc1 = sc.fit_transform(stocks.drop(columns = ['symbol', 'date']))         





pipe = Pipeline([
    ('km', KMeans(random_state = 2024))
])





params = {
    'km__n_clusters': [*range(3, 12)],
    'km__n_init': [10, 20, 30]
}





gs1 = GridSearchCV(
    pipe,
    param_grid = params,
    n_jobs = -1,
    verbose = 1
)








# fit gridsearch with all numeric columns
# except corr_weighted_vol_vs_volatility
gs1.fit(stocks.drop(columns = ['symbol', 'date']))


gs1.best_params_


# fit KMeans model based on the best parameters
km1 = KMeans(
    n_clusters = gs1.best_params_['km__n_clusters'],
    n_init = gs1.best_params_['km__n_init'],
    random_state = 2024
)

km1.fit(stocks_sc1)


# get cluster labels
set(km1.labels_)


# check inertia score
km1.inertia_


# check silhouette score
silhouette_score(stocks_sc1, km1.labels_)


# store results
results = {
    'clusters': gs1.best_params_['km__n_clusters'],
    'n_init': gs1.best_params_['km__n_init'],
    'inertia': round(km1.inertia_, 0),
    'silhouette': round(silhouette_score(stocks_sc1, km1.labels_), 3),
    'remarks': 'Cluster A: All numeric features',
    'reference': 'stocks_sc1, gs1, km1'
}

metrics.append(results)


# add clusters to original dataset
stocks['cluster_a'] = km1.labels_

# confirm
stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_a')['symbol'].nunique()














# standard scaler is already instantiated as sc
# scale data, just three engineered features
stocks_sc2 = sc.fit_transform(stocks[['price_range', 'volatility', 'price_change_percentage']])         





gs2 = GridSearchCV(
    pipe,
    param_grid = params,
    n_jobs = -1,
    verbose = 1
)








gs2.fit(stocks[['price_range', 'volatility', 'price_change_percentage']])


gs2.best_params_


# fit KMeans model based on the best parameters
km2 = KMeans(
    n_clusters = gs2.best_params_['km__n_clusters'],
    n_init = gs2.best_params_['km__n_init'],
    random_state = 2024
)

km2.fit(stocks_sc2)


set(km2.labels_)


km2.inertia_


silhouette_score(stocks_sc2, km2.labels_)


# store results
results = {
    'clusters': gs2.best_params_['km__n_clusters'],
    'n_init': gs2.best_params_['km__n_init'],
    'inertia': round(km2.inertia_, 0),
    'silhouette': round(silhouette_score(stocks_sc2, km2.labels_), 3),
    'remarks': 'Cluster B: Three engineered features',
    'reference': 'stocks_sc2, gs2, km2'
}

metrics.append(results)


# add clusters to original dataset
stocks['cluster_b'] = km2.labels_

# confirm
stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_b')['symbol'].nunique()














# standard scaler is already instantiated as sc
# scale data, just three engineered features
stocks_sc3 = sc.fit_transform(stocks[['price_range', 'volatility']])         





gs3 = GridSearchCV(
    pipe,
    param_grid = params,
    n_jobs = -1,
    verbose = 1
)








gs3.fit(stocks[['price_range', 'volatility']])


gs3.best_params_


# fit KMeans model based on the best parameters
km3 = KMeans(
    n_clusters = gs3.best_params_['km__n_clusters'],
    n_init = gs3.best_params_['km__n_init'],
    random_state = 2024
)

km3.fit(stocks_sc3)


set(km3.labels_)


km3.inertia_


silhouette_score(stocks_sc3, km3.labels_)


# store results
results = {
    'clusters': gs3.best_params_['km__n_clusters'],
    'n_init': gs3.best_params_['km__n_init'],
    'inertia': round(km3.inertia_, 0),
    'silhouette': round(silhouette_score(stocks_sc3, km3.labels_), 3),
    'remarks': 'Cluster C: Best Two engineered features',
    'reference': 'stocks_sc3, gs3, km3'
}

metrics.append(results)


# add clusters to original dataset
stocks['cluster_c'] = km3.labels_

# confirm
stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_c')['symbol'].nunique()





# loop thru various epsilon values to see how clusters change, and to reduce outliers
# default value for epsilon is 0.5, test values between 0.1 and 1.0

epsilon_values_1 = [*np.linspace(0.1, 1.0, 10)]
number_of_clusters_1 = []
number_of_outliers_1 = []
sil_scores_1 = []

for eps in epsilon_values_1:
    
    # instantiate model
    dbscan = DBSCAN(eps = eps)
    
    # fit model
    dbscan.fit(stocks_sc1)

    # count and store clusters
    number_of_clusters_1.append(len(set(dbscan.labels_)))

    # calculate and store silhouette score
    sil_scores_1.append(round(silhouette_score(stocks_sc1, dbscan.labels_), 3))
    
    # check and count outliers
    if -1 in set(dbscan.labels_):
        number_of_outliers_1.append((dbscan.labels_ == -1).sum())

print('Done waiting...')
print(f'Clusters: {number_of_clusters_1}')
print(f'Outliers: {number_of_outliers_1}')
print(f'Silhouette Scores: {sil_scores_1}')


# check silhouette scores vs. epsilon

plt.figure(figsize = (8, 4))

plt.plot(epsilon_values_1, sil_scores_1, color = '#003366', marker = 'o', linestyle = ':')

plt.xticks(epsilon_values_1)
plt.xlabel('Epsilon Values')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score per Epsilon Value');

plt.savefig('../images/silhouette_model4a.png');





# loop thru various epsilon values to see how clusters change, and to reduce outliers
# default value for epsilon is 0.5, test values between 1.1 and 1.5

epsilon_values_2 = [*np.linspace(1.1, 1.5, 5)]
number_of_clusters_2 = []
number_of_outliers_2 = []
sil_scores_2 = []

for eps in epsilon_values_2:
    
    # instantiate model
    dbscan = DBSCAN(eps = eps)
    
    # fit model
    dbscan.fit(stocks_sc1)

    # count and store clusters
    number_of_clusters_2.append(len(set(dbscan.labels_)))

    # calculate and store silhouette score
    sil_scores_2.append(round(silhouette_score(stocks_sc1, dbscan.labels_), 3))
    
    # check and count outliers
    if -1 in set(dbscan.labels_):
        number_of_outliers_2.append((dbscan.labels_ == -1).sum())

print('Done waiting...')
print(f'Clusters: {number_of_clusters_2}')
print(f'Outliers: {number_of_outliers_2}')
print(f'Silhouette Scores: {sil_scores_2}')


# check silhouette scores vs. epsilon

plt.figure(figsize = (8, 4))

plt.plot(epsilon_values_2, sil_scores_2, color = '#003366', marker = 'o', linestyle = ':')

plt.xticks(epsilon_values_2)
plt.xlabel('Epsilon Values')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score per Epsilon Value');

plt.savefig('../images/silhouette_model4b.png');








dbs1 = DBSCAN(eps = 1.3)
dbs1.fit(stocks_sc1)


set(dbs1.labels_)


# store results
results = {
    'clusters': len(set(dbs1.labels_)),
    'n_init': np.nan,
    'inertia': np.nan,
    'silhouette': round(silhouette_score(stocks_sc1, dbs1.labels_), 3),
    'remarks': 'Cluster D: All numeric features',
    'reference': 'stocks_sc1, dbs1'
}

metrics.append(results)


stocks['cluster_d'] = dbs1.labels_


stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_d')['symbol'].nunique()





# loop thru various epsilon values to see how clusters change, and to reduce outliers
# default value for epsilon is 0.5, test values between 0.1 and 1.0

epsilon_values_3 = [*np.linspace(0.1, 1.0, 10)]
number_of_clusters_3 = []
number_of_outliers_3 = []
sil_scores_3 = []

for eps in epsilon_values_3:
    
    # instantiate model
    dbscan = DBSCAN(eps = eps)
    
    # fit model
    dbscan.fit(stocks_sc2)

    # count and store clusters
    number_of_clusters_3.append(len(set(dbscan.labels_)))

    # calculate and store silhouette score
    sil_scores_3.append(round(silhouette_score(stocks_sc2, dbscan.labels_), 3))
    
    # check and count outliers
    if -1 in set(dbscan.labels_):
        number_of_outliers_3.append((dbscan.labels_ == -1).sum())

print('Done waiting...')
print(f'Clusters: {number_of_clusters_3}')
print(f'Outliers: {number_of_outliers_3}')
print(f'Silhouette Scores: {sil_scores_3}')


# check silhouette scores vs. epsilon

plt.figure(figsize = (8, 4))

plt.plot(epsilon_values_3, sil_scores_3, color = '#003366', marker = 'o', linestyle = ':')

plt.xticks(epsilon_values_3)
plt.xlabel('Epsilon Values')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score per Epsilon Value');

plt.savefig('../images/silhouette_model5.png');





dbs2 = DBSCAN(eps = 1.0)
dbs2.fit(stocks_sc2)


set(dbs2.labels_)


# store results
results = {
    'clusters': len(set(dbs2.labels_)),
    'n_init': np.nan,
    'inertia': np.nan,
    'silhouette': round(silhouette_score(stocks_sc2, dbs2.labels_), 3),
    'remarks': 'Cluster E: Three engineered features',
    'reference': 'stocks_sc2, dbs2'
}

metrics.append(results)


stocks['cluster_e'] = dbs2.labels_
stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_e')['symbol'].nunique()





# loop thru various epsilon values to see how clusters change, and to reduce outliers
# default value for epsilon is 0.5, test values between 0.1 and 1.0

epsilon_values_4 = [*np.linspace(0.1, 1.0, 10)]
number_of_clusters_4 = []
number_of_outliers_4 = []
sil_scores_4 = []

for eps in epsilon_values_4:
    
    # instantiate model
    dbscan = DBSCAN(eps = eps)
    
    # fit model
    dbscan.fit(stocks_sc3)

    # count and store clusters
    number_of_clusters_4.append(len(set(dbscan.labels_)))

    # calculate and store silhouette score
    sil_scores_4.append(round(silhouette_score(stocks_sc3, dbscan.labels_), 3))
    
    # check and count outliers
    if -1 in set(dbscan.labels_):
        number_of_outliers_4.append((dbscan.labels_ == -1).sum())

print('Done waiting...')
print(f'Clusters: {number_of_clusters_4}')
print(f'Outliers: {number_of_outliers_4}')
print(f'Silhouette Scores: {sil_scores_4}')


# check silhouette scores vs. epsilon

plt.figure(figsize = (8, 4))

plt.plot(epsilon_values_4, sil_scores_4, color = '#003366', marker = 'o', linestyle = ':')

plt.xticks(epsilon_values_4)
plt.xlabel('Epsilon Values')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score per Epsilon Value');

plt.savefig('../images/silhouette_model6.png');





dbs3 = DBSCAN(eps = 0.4)
dbs3.fit(stocks_sc3)


set(dbs3.labels_)


# store results
results = {
    'clusters': len(set(dbs3.labels_)),
    'n_init': np.nan,
    'inertia': np.nan,
    'silhouette': round(silhouette_score(stocks_sc3, dbs3.labels_), 3),
    'remarks': 'Cluster F: Two engineered features',
    'reference': 'stocks_sc3, dbs3'
}

metrics.append(results)


stocks['cluster_f'] = dbs3.labels_
stocks.head()


# check number of unique stocks per cluster
stocks.groupby('cluster_f')['symbol'].nunique()





pd.DataFrame(metrics)


stocks_by_avg_vol = stocks.groupby('symbol')['volatility'].mean().to_dict()


# save these values in variable v
v = stocks.groupby('symbol')['volatility'].mean().describe(percentiles = [.75, .9])
v


print(f"High volatility (high risk) above {round(v['90%'], 2)}")
print(f"Medium volatility (medium risk) between {round(v['75%'], 2)} and below {round(v['90%'], 2)}")
print(f"Low volatility (low risk) below {round(v['75%'], 2)}")


high_risk = []
medium_risk = []
low_risk = []

for key, value in stocks_by_avg_vol.items():
    if value > v['90%']:
        high_risk.append(key)
    elif value > v['75%'] and value <= v['90%']:
        medium_risk.append(key)
    else:
        low_risk.append(key)

print(f"Number of high risk stocks: {len(high_risk)}")
print(f"Number of medium risk stocks: {len(medium_risk)}")
print(f"Number of low risk stocks: {len(low_risk)}")





def build_portfolios(cluster):
    
    count = stocks[cluster].nunique()
    suffix = cluster[-1]
    
    for i in range(count):
    
        averages = []
        
        if suffix in 'abc':
            for stock in stocks[stocks[cluster] == i]['symbol'].unique():
                averages.append(stocks_by_avg_vol[stock])
                avg = np.mean(averages)
        
        else:
            for stock in stocks[stocks[cluster] == i-1]['symbol'].unique():
                averages.append(stocks_by_avg_vol[stock])
                avg = np.mean(averages)
            
        if avg > v['90%']:
            portfolio = 'high risk'
        elif avg > v['75%'] and value <= v['90%']:
            portfolio = 'medium risk'
        else:
            portfolio = 'low risk'
          
        print(f'Porfolio: {i+1}, Avg. volatility: {round(avg, 2)}')
        print(f'Number of stocks: {len(averages)}, Type: {portfolio}')
        print('-'*40)


build_portfolios('cluster_d')
